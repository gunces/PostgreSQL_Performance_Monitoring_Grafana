# Global Agent Configuration
[agent]
  hostname = "<change it>"
  flush_interval = "10s"
  interval = "10s"
  debug = true
  #logfile = "/var/log/telegraf/telegraf.log"
  logfile = "/dev/null"
  #logfile_rotation_interval = "7d"
  #logfile_rotation_max_size = "1GB"

# Input Plugins

[[inputs.cpu]]
    percpu = true
    totalcpu = true
    collect_cpu_time = false
    report_active = false
[[inputs.disk]]
    ignore_fs = ["tmpfs", "devtmpfs", "devfs"]
[[inputs.io]]
[[inputs.mem]]
[[inputs.net]]
[[inputs.system]]
[[inputs.swap]]
[[inputs.netstat]]
[[inputs.processes]]
[[inputs.kernel]]


[[inputs.postgresql_extensible]]
  address = "host=localhost user=telegraf port=<change it> dbname=postgres"
  databases=["postgres"]

  [[inputs.postgresql_extensible.query]]
    measurement="postgresql.pg_stat_database_dtl"
    sqlquery="select d.datname, coalesce(saa.all_sessions, 0) as all_sessions, coalesce(saa.active, 0) as \"active\", coalesce(saa.idle, 0) as \"idle\", coalesce(saa.\"idle in transaction\", 0) as \"idle in transaction\", coalesce(saa.number_of_conn, 0) as number_of_conn, (select (xact_commit+xact_rollback) as tps_number from pg_stat_database where datname=d.datname)  as tps_number, pg_database_size(d.datname) as datsize_in_kb from pg_database d left join (select sa.datname,count(1) as all_sessions,count(case when sa.state='active' then 1 end) as active,count(case when sa.state='idle' then 1 end) as idle, count(case when sa.state='idle in transaction' then 1 end) as \"idle in transaction\",(SELECT sum(numbackends) as number_of_conn FROM pg_stat_database where datname=sa.datname) as number_of_conn from pg_stat_activity sa where sa.datname not in ('template1','template0') group by sa.datname) saa on saa.datname=d.datname where d.datname not in ('template1','template0')"
    version = 900
    withdbname=false

  [[inputs.postgresql_extensible.query]]
    measurement="postgresql.pg_general_information"
    sqlquery="select (now() - pg_postmaster_start_time()) as uptime, (substring(version() from position(' ' in version())+1 for (position('on' in version())-position(' ' in version())-4)))::int as pg_version, (select setting::int from pg_settings where name = 'shared_buffers') as shared_buffers, (select setting::int from pg_settings where name = 'max_connections') as max_connections, (select setting::int from pg_settings where name = 'effective_cache_size') as effective_cache_size, (select setting::int from pg_settings where name = 'maintenance_work_mem') as maintenance_work_mem, (select setting::int from pg_settings where name = 'work_mem') as work_mem, (select setting::int from pg_settings where name = 'max_wal_size') as max_wal_size, (select setting::int from pg_settings where name = 'max_worker_processes') as max_worker_processes, (select setting::int from pg_settings where name = 'max_parallel_workers') as max_parallel_workers"
    version = 910
    withdbname=false

  [[inputs.postgresql_extensible.query]]
    measurement = "postgresql.pg_stat_database"
    sqlquery = "select datname, numbackends, xact_commit, xact_rollback, blks_read, blks_hit, tup_returned, tup_fetched, tup_inserted, tup_updated, tup_deleted, deadlocks, temp_files, temp_bytes , round(sum(blks_hit)*100/sum(blks_hit+blks_read))::int as cache_hit_ratio from pg_stat_database where datname not in ('template1','template0') group by datname, numbackends, xact_commit, xact_rollback, blks_read, blks_hit, tup_returned, tup_fetched, tup_inserted, tup_updated, tup_deleted, deadlocks, temp_files, temp_bytes"
    version = 910
    withdbname = false

  [[inputs.postgresql_extensible.query]]
    measurement = "postgresql.pg_most50_calls_queries"
    sqlquery = "SELECT  pg_user.usename as username, pg_stat_database.datname, regexp_replace(query, '\\r|\\n|\\t|\\s+', ' ', 'g') as qry, queryid, calls, total_time, min_time, max_time, mean_time, stddev_time, rows, shared_blks_hit, shared_blks_read, shared_blks_dirtied, shared_blks_written, local_blks_hit, local_blks_read, local_blks_dirtied, local_blks_written, temp_blks_read, temp_blks_written, pg_stat_statements.blk_read_time, pg_stat_statements.blk_write_time FROM pg_stat_statements JOIN pg_user ON (pg_user.usesysid = pg_stat_statements.userid) JOIN pg_stat_database ON (pg_stat_database.datid = pg_stat_statements.dbid) order by calls desc limit 50"
    tagvalue = "userid,dbid,queryid,qry,usename,datname"
    version = 910
    withdbname = false

  [[inputs.postgresql_extensible.query]]
    measurement = "postgresql.pg_most50_time_consuming_queries"
    sqlquery = "SELECT  pg_user.usename as username, pg_stat_database.datname, regexp_replace(query, '\\r|\\n|\\t|\\s+', ' ', 'g') as qry, queryid, calls, total_time, min_time, max_time, mean_time, stddev_time, rows, shared_blks_hit, shared_blks_read, shared_blks_dirtied, shared_blks_written, local_blks_hit, local_blks_read, local_blks_dirtied, local_blks_written, temp_blks_read, temp_blks_written, pg_stat_statements.blk_read_time, pg_stat_statements.blk_write_time FROM pg_stat_statements JOIN pg_user ON (pg_user.usesysid = pg_stat_statements.userid) JOIN pg_stat_database ON (pg_stat_database.datid = pg_stat_statements.dbid) order by total_time desc limit 50"
    tagvalue = "userid,dbid,queryid,qry,usename,datname"
    version = 910
    withdbname = false


  [[inputs.postgresql_extensible.query]]
    measurement = "postgresql.pg_avg_query_time"
    sqlquery = "select round((sum(total_time) / sum(calls))::numeric,2) as avg_query_time from pg_stat_statements"
    version = 910
    withdbname = false

  [[inputs.postgresql_extensible.query]]
    measurement = "postgresql.pg_stat_bgwriter"
    sqlquery = "select checkpoints_timed, checkpoints_req, checkpoint_write_time, checkpoint_sync_time, buffers_checkpoint, buffers_clean, maxwritten_clean, buffers_backend from pg_stat_bgwriter"
    version = 910
    withdbname = false

  [[inputs.postgresql_extensible.query]]
    measurement = "postgresql.pg_check_replication_health"
    sqlquery = "select hfr.address as replica_ip_in_hba,sr.client_addr as replica_ip_in_stat,(case when sr.client_addr is null then false else true end)::text as is_reachable, usename, backend_start, sent_lsn, write_lag,flush_lag, replay_lag, sync_state, pg_wal_lsn_diff(sent_lsn,replay_lsn) as diff_in_bytes from pg_hba_file_rules hfr left join pg_stat_replication sr on hfr.address::text = substring(sr.client_addr::text from 0 for position('/' in sr.client_addr::text)) where hfr.database = '{replication}'"
    tagvalue = "replica_ip_in_hba,replica_ip_in_stat,usename,backend_start,sent_lsn,write_lag,flush_lag,sync_state,diff_in_bytes"
    version = 910
    withdbname = false

  [[inputs.postgresql_extensible.query]]
    measurement = "postgresql.pg_stat_activity_active"
    sqlquery = "select sa.pid, sa.datname, sa.usename, sa.application_name, sa.client_addr, sa.backend_start::text as bck_start, sa.xact_start::text, sa.query_start::text as qry_start, sa.wait_event_type, sa.wait_event, sa.state, regexp_replace(sa.query, '\\r|\\n|\\t|\\s+', ' ', 'g') as qry, sa.backend_type from pg_stat_activity sa where sa.state = 'active' and pid<>pg_backend_pid()"
    version = 910
    tagvalue = "pid,datname,usename,application_name"
    withdbname = false

  [[inputs.postgresql_extensible.query]]
    measurement = "postgresql.block_detail"
    sqlquery = "SELECT pg_stat_database.datname, sum(calls)::float as calls, sum(total_time)::float as total_time, sum(rows)::float as rows, sum(shared_blks_hit)::float as shared_blks_hit, sum(shared_blks_read)::float as shared_blks_read, sum(shared_blks_dirtied)::float as shared_blks_dirtied, sum(shared_blks_written)::float as shared_blks_written, sum(local_blks_hit)::float as local_blks_hit, sum(local_blks_read)::float as local_blks_read, sum(local_blks_dirtied)::float as local_blks_dirtied, sum(local_blks_written)::float as local_blks_written, sum(temp_blks_read)::float as temp_blks_read, sum(temp_blks_written)::float as temp_blks_written, sum(pg_stat_statements.blk_read_time)::float as blk_read_time, sum(pg_stat_statements.blk_write_time)::float as blk_write_time FROM pg_stat_statements JOIN pg_user ON (pg_user.usesysid = pg_stat_statements.userid) JOIN pg_stat_database ON (pg_stat_database.datid = pg_stat_statements.dbid) GROUP BY pg_stat_database.datname"
    version = 910
    tagvalue = "datname"
    withdbname = false

  [[inputs.postgresql_extensible.query]]
    measurement = "postgresql.pg_blocking_queries"
    sqlquery = "select * from dba.pg_blocking_queries"
    version = 910
    tagvalue = "datname,blocked_pid,blocked_user,blocking_pid,blocking_user,blocked_statement,current_statement_in_blocking_process,blocked_application,blocking_application"
    withdbname = false


# Output Plugin InfluxDB
[[inputs.file]]
  files = ["/etc/telegraf/DONT_DELETE/allTablesStatistics.csv"]
  data_format = "csv"
  csv_header_row_count = 0
  csv_column_names = ["measurement","datname","host","server","schema_name","table_name","rel_size","index_size","seq_scan","seq_tup_read","idx_scan","idx_tup_fetch","n_tup_ins","n_tup_upd","n_tup_del","n_tup_hot_upd","n_live_tup","n_dead_tup","n_mod_since_analyze","last_vacuum","last_autovacuum","last_analyze","last_autoanalyze","vacuum_count","autovacuum_count","analyze_count","autoanalyze_count","time"]
  csv_column_types = ["string","string","string","string","string","string","int","int","int","int","int","int","int","int","int","int","int","int","int","string","string","string","string","int","int","int","int","time"]
  csv_delimiter = ","
  csv_tag_columns = ["datname","host","schema_name","table_name"]
  csv_measurement_column = "measurement"
  csv_timestamp_column = "time"
  csv_timestamp_format = "unix_ns"

[[inputs.file]]
  files = ["/etc/telegraf/DONT_DELETE/bloats.csv"]
  data_format = "csv"
  csv_header_row_count = 0
  csv_column_names = ["measurement","datname","host","server","type","schemaname","relname","bloat","wasted","object_size","time"]
  csv_column_types = ["string","string","string","string","string","string","string","float","float","float","time"]
  csv_delimiter = ","
  csv_tag_columns = ["datname","host","schemaname","relname"]
  csv_measurement_column = "measurement"
  csv_timestamp_column = "time"
  csv_timestamp_format = "unix_ns"

[[inputs.file]]
  files = ["/etc/telegraf/DONT_DELETE/dbServerInfo.csv"]
  data_format = "csv"
  csv_header_row_count = 0
  csv_column_names = ["measurement","host","server","ip","isDbUp","dbVersion","diskDataSize","diskDataUsed","diskDataPercentage","isDbPrimary","time"]
  csv_column_types = ["string","string","string","string","string","string","float","float","float","string","time"]
  csv_delimiter = ","
  csv_tag_columns = ["host","server","ip"]
  csv_measurement_column = "measurement"
  csv_timestamp_column = "time"
  csv_timestamp_format = "unix_ns"

[[inputs.file]]
  files = ["/etc/telegraf/DONT_DELETE/vacuumProgress.csv"]
  data_format = "csv"
  csv_header_row_count = 0
  csv_column_names = ["measurement","during_min","blks_left","relname","scanned_perc","vacuum_perc","pid","phase","heap_blks_total","heap_blks_scanned","heap_blks_vacuumed","index_vacuum_count","max_dead_tuples","num_dead_tuples","time"]
  csv_column_types = ["string","float","float","string","float","float","int","string","float","float","float","float","float","float","time"]
  csv_delimiter = ","
  csv_tag_columns = ["pid","relname"]
  csv_measurement_column = "measurement"
  csv_timestamp_column = "time"
  csv_timestamp_format = "unix_ns"


# Output Plugin InfluxDB
[[outputs.influxdb]]
  database = "postgres"
  urls = [ "http://<change ip>:<change port>" ]
  username = "telegraf"
  password = "<change password>"
